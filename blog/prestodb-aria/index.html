<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A boutique big data solutions company specializing in data architecture and engineering with open-source and cloud technologies"><meta name=Author content="Datacatessen, LLC"><meta name=keywords content="hugo blog"><link rel=stylesheet href=https://datacatessen.com/css/syntax.css><link rel=stylesheet href=https://datacatessen.com/css/style.css><script src=https://kit.fontawesome.com/1b7478c139.js crossorigin=anonymous></script><title></title></head><body><aside id=sidenav><header><a href=https://datacatessen.com><img src=https://datacatessen.com/avatar.png alt=avatar></a></header><nav><a href=/><i class="fas fa-home fa-sm"></i><span>home</span></a>
<a href=/blog/><i class="fas fa-keyboard fa-ms"></i><span>blog</span></a>
<a href=/services><i class="fas fa-info-circle"></i><span>services</span></a>
<a href=/tags><i class="fas fa-tags fa-ms"></i><span>tags</span></a>
<a href=/contact><i class="far fa-envelope"></i><span>contact</span></a></nav></aside><main id=main><a href=javascript:void(0) id=closebtn onclick=navToggle()><i class="fas fa-bars fa-lg"></i></a><div class="content main-bg-image"><h1 id=title>Getting Started with PrestoDB and Aria Scan Optimizations</h1><nav id=TableOfContents></nav><p><a href=https://prestodb.io target=_blank>PrestoDB</a>
recently released a set of experimental features under their Aria project in order to increase table scan performance of data stored in ORC files via the Hive Connector. In this post, we&rsquo;ll check out these new features at a very basic level using a test environment of PrestoDB on Docker. To find out more about the Aria features, you can check out the <a href=https://engineering.fb.com/data-infrastructure/aria-presto/ target=_blank>Facebook Engineering</a>
blog post which was published June 2019.</p><p>Presto is a massively parallel processing (MPP) SQL execution engine. The execution engine is decoupled from data storage, and the project contains numerous plugins, called <em>Connectors</em>, that provide the Presto engine with data for query execution. Data is read from the data store, then handed to Presto where it takes over to perform the operations of the query, such as joining data and performing aggregations. This decoupling of data storage and execution allows for a single Presto instance to query various data sources, providing a very powerful federated query layer. There are many connectors available for Presto, and the community regularly provides additional connectors for data stores.</p><p>The Hive Connector is often considered the standard connector for Presto. This connector is configured to connect to a Hive metastore, which exposes metadata about the tables defined in the metastore. Data is typically stored in HDFS or S3, and the metastore provides information about where the files are stored and in what format, typically ORC but there are other supported formats such as Avro and Parquet. The Hive connector allows the Presto engine to scan data from HDFS/S3 in parallel into the engine to execute your query. <a href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC target=_blank>ORC (Optimized Row Columnar)</a>
format is a very standard and common format for storing data, as it provides good compression and performance.</p><p>Presto has two core services for executing queries. A <em>Coordinator</em>, which is responsible for query parsing and scheduling (among other things), and many <em>Workers</em> which execute the queries in parallel. The Coordinator can also act as a Worker, though it is not used for production environments. Since we&rsquo;re playing with Presto here, we&rsquo;ll just use one node to act as both a Coordinator and Worker. More detailed documentation, including installation details, can be found <a href=https://prestodb.io/docs/current/ target=_blank>here</a>
.</p><p>Let&rsquo;s take a look at getting a Docker image together for Presto (though they already exist on Dockerhub, e.g. <a href=https://hub.docker.com/r/ahanaio/prestodb-sandbox target=_blank>ahanaio/prestodb-sandbox</a>
). We can see below how relatively easy it is to get Presto up and running. We download Presto, copy some configuration files in a local <code>etc</code> directory into the image, and specify an entry point to run the server.</p><div class=highlight><pre class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=k>FROM</span><span class=s> openjdk:8-jre</span><span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Pick our Presto Version and the URL to download</span><span class=err>
</span><span class=err></span><span class=k>ARG</span> <span class=nv>PRESTO_VERSION</span><span class=o>=</span><span class=m>0</span>.235.1<span class=err>
</span><span class=err></span><span class=k>ARG</span> <span class=nv>PRESTO_BIN</span><span class=o>=</span>https://repo1.maven.org/maven2/com/facebook/presto/presto-server/<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>/presto-server-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>.tar.gz<span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Update the base image OS and install wget and python</span><span class=err>
</span><span class=err></span><span class=k>RUN</span> apt-get update<span class=err>
</span><span class=err></span><span class=k>RUN</span> apt-get install -y wget python<span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Download Presto and unpack it to /opt/presto</span><span class=err>
</span><span class=err></span><span class=k>RUN</span> wget --quiet <span class=si>${</span><span class=nv>PRESTO_BIN</span><span class=si>}</span><span class=err>
</span><span class=err></span><span class=k>RUN</span> mkdir -p /opt<span class=err>
</span><span class=err></span><span class=k>RUN</span> tar -xf presto-server-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>.tar.gz -C /opt<span class=err>
</span><span class=err></span><span class=k>RUN</span> rm presto-server-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>.tar.gz<span class=err>
</span><span class=err></span><span class=k>RUN</span> ln -s /opt/presto-server-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span> /opt/presto<span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Copy configuration files on the host into the image</span><span class=err>
</span><span class=err></span><span class=k>COPY</span> etc /opt/presto/etc<span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Download the Presto CLI and put it in the image</span><span class=err>
</span><span class=err></span><span class=k>RUN</span> wget --quiet https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>/presto-cli-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>-executable.jar<span class=err>
</span><span class=err></span><span class=k>RUN</span> mv presto-cli-<span class=si>${</span><span class=nv>PRESTO_VERSION</span><span class=si>}</span>-executable.jar /usr/local/bin/presto<span class=err>
</span><span class=err></span><span class=k>RUN</span> chmod +x /usr/local/bin/presto<span class=err>
</span><span class=err>
</span><span class=err></span><span class=c># Specify the entrypoint to start</span><span class=err>
</span><span class=err></span><span class=k>ENTRYPOINT</span> /opt/presto/bin/launcher run<span class=err>
</span></code></pre></div><p>There are four files in the <code>etc</code> folder to configure Presto, along with two catalogs. Details of them can be found <a href=https://prestodb.io/docs/current/installation/deployment.html target=_blank>here</a>
. They are:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>etc/
├── catalog
│   ├── hive.properties  <span class=c1># Defines configuration properties for our Hive Metastore</span>
│   └── tpch.properties  <span class=c1># Enables the TPCH connector to generate data</span>
├── config.properties 	 <span class=c1># Presto instance configuration properties</span>
├── jvm.config           <span class=c1># JVM configuration for the process</span>
├── log.properties       <span class=c1># Logging configuration</span>
└── node.properties      <span class=c1># Node-specific configuration properties</span>
</code></pre></div><p>The four files directly under <code>etc</code> are documented in the above link. Let&rsquo;s look at the details of the two catalogs: <code>hive</code> and <code>tpch</code>. The former will be used to read data, and the latter will be used to generate data into our Hive warehouse to run queries. Setting up Hive is outside the scope of this document, but will be covered in a future post. In short, you need a catalog service in order to use the Hive connector, such as a Hive Metastore or AWS Glue. The catalog service contains all of the table definitions used by Presto, and data is typically stored in HDFS or S3. We&rsquo;ve set up a Dockerized Hive Metastore and single-node HDFS instance.</p><p><code>etc/catalog/hive.properties</code></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>connector.name<span class=o>=</span>hive-hadoop2
hive.metastore.uri<span class=o>=</span>thrift://localhost:9083/hivemetastore
hive.metastore.username<span class=o>=</span>hive
hive.allow-drop-table<span class=o>=</span><span class=nb>true</span>
</code></pre></div><p><code>etc/catalog/tpch.properties</code></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>connector.name<span class=o>=</span>tpch
</code></pre></div><p>We&rsquo;re now ready to build our Docker container and start Presto using <code>host</code> networking so it can talk to the metastore and HDFS which is running on the host (also in Docker).</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker build . -t prestodb:latest
docker run --name presto --network host prestodb:latest
</code></pre></div><p>We&rsquo;ll use the <a href=https://prestodb.io/docs/current/installation/cli.html target=_blank>Presto CLI</a>
to connect to Presto that we put inside the image.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker <span class=nb>exec</span> -it presto presto
</code></pre></div><p>Next, we will create our schema to hold the TPC-H data set and then create the tables. By default, Presto will create ORC files, which is convenient for us since that is what we&rsquo;re looking at testing. We are using the <code>sf100</code> TPC-H schema to create a data set of about 23 GB total. This should give us enough data to show off the Aria enhancements. (The <code>sf</code> stands for <em>scale factor</em> and is about 100 times larger than the <code>sf1</code> schema.)</p><div class=highlight><pre class=chroma><code class=language-SQL data-lang=SQL><span class=k>CREATE</span> <span class=k>SCHEMA</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>;</span>

<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>customer</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>customer</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>lineitem</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>lineitem</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>nation</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>nation</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>orders</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>orders</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>part</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>part</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>partsupp</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>partsupp</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>region</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>region</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>TABLE</span> <span class=n>hive</span><span class=p>.</span><span class=n>tpch</span><span class=p>.</span><span class=n>supplier</span> <span class=k>AS</span> <span class=k>SELECT</span> <span class=o>*</span> <span class=k>FROM</span> <span class=n>tpch</span><span class=p>.</span><span class=n>sf100</span><span class=p>.</span><span class=n>supplier</span><span class=p>;</span>

<span class=n>USE</span> <span class=n>tpch</span><span class=p>;</span>
</code></pre></div><p>Let&rsquo;s talk a bit about how Presto executes a query. The Presto Coordinator parses the query to build a plan (which we will see examples of below). Once the plan is made, it is broken into several stages (or fragments) which execute a series of operators. Operators are a particular function that the engine performs to execute your query. This typically begins with scanning data via a Connector, then performing operations such as filtering data, partial aggregations, and commonly exchanging data between Presto workers to perform joins and final aggregations. All of these stages are broken into <em>splits</em>, which is a unit of parallelism in Presto. Workers execute a configurable number of splits in parallel to get your desired results. All data in the engine is kept in-memory (as long as you don&rsquo;t go past the thresholds of the cluster; another topic for another time).</p><p>The Hive connector (and all connectors for that matter) are responsible for breaking the input data set into splits for Presto to read in parallel. As an optimization, the Presto engine will tell connectors the predicates used in a query and what columns are being selected, called <em>predicate pushdown</em>, which enables connectors to drop data before even handing it to the engine (what this blog post is all about!).</p><p>To demonstrate the predicate pushdown, let&rsquo;s take a look at a basic query &ndash; counting rows of a table within a range. The TPC-H <code>lineitem</code> table has about 600 million rows with a <code>shipdate</code> between 1992 and 1998. Let&rsquo;s start without enabling the session properties to enable the Aria enhancements, running an <code>EXPLAIN</code> command to take a look at the query plan.</p><div class=highlight><pre class=chroma><code class=language-SQL data-lang=SQL><span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>EXPLAIN</span> <span class=p>(</span><span class=k>TYPE</span> <span class=n>DISTRIBUTED</span><span class=p>)</span> <span class=k>SELECT</span> <span class=k>COUNT</span><span class=p>(</span><span class=n>shipdate</span><span class=p>)</span> <span class=k>FROM</span> <span class=n>lineitem</span> <span class=k>WHERE</span> <span class=n>shipdate</span> <span class=k>BETWEEN</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-01-01&#39;</span> <span class=k>AND</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-12-31&#39;</span><span class=p>;</span>

<span class=n>Fragment</span> <span class=mi>0</span> <span class=p>[</span><span class=n>SINGLE</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>layout</span><span class=p>:</span> <span class=p>[</span><span class=k>count</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>partitioning</span><span class=p>:</span> <span class=n>SINGLE</span> <span class=p>[]</span>
    <span class=n>Stage</span> <span class=n>Execution</span> <span class=n>Strategy</span><span class=p>:</span> <span class=n>UNGROUPED_EXECUTION</span>
    <span class=o>-</span> <span class=k>Output</span><span class=p>[</span><span class=n>_col0</span><span class=p>]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=k>count</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
            <span class=n>_col0</span> <span class=p>:</span><span class=o>=</span> <span class=k>count</span>
        <span class=o>-</span> <span class=k>Aggregate</span><span class=p>(</span><span class=k>FINAL</span><span class=p>)</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=k>count</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
                <span class=k>count</span> <span class=p>:</span><span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=n>presto</span><span class=p>.</span><span class=k>default</span><span class=p>.</span><span class=k>count</span><span class=s2>&#34;&#34;</span><span class=p>((</span><span class=n>count_4</span><span class=p>))</span>
            <span class=o>-</span> <span class=n>LocalExchange</span><span class=p>[</span><span class=n>SINGLE</span><span class=p>]</span> <span class=p>()</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
                <span class=o>-</span> <span class=n>RemoteSource</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>

<span class=n>Fragment</span> <span class=mi>1</span> <span class=p>[</span><span class=k>SOURCE</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>layout</span><span class=p>:</span> <span class=p>[</span><span class=n>count_4</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>partitioning</span><span class=p>:</span> <span class=n>SINGLE</span> <span class=p>[]</span>
    <span class=n>Stage</span> <span class=n>Execution</span> <span class=n>Strategy</span><span class=p>:</span> <span class=n>UNGROUPED_EXECUTION</span>
    <span class=o>-</span> <span class=k>Aggregate</span><span class=p>(</span><span class=k>PARTIAL</span><span class=p>)</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
            <span class=n>count_4</span> <span class=p>:</span><span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=n>presto</span><span class=p>.</span><span class=k>default</span><span class=p>.</span><span class=k>count</span><span class=s2>&#34;&#34;</span><span class=p>((</span><span class=n>shipdate</span><span class=p>))</span>
        <span class=o>-</span> <span class=n>ScanFilter</span><span class=p>[</span><span class=k>table</span> <span class=o>=</span> <span class=n>TableHandle</span> <span class=err>{</span><span class=n>connectorId</span><span class=o>=</span><span class=s1>&#39;hive&#39;</span><span class=p>,</span> <span class=n>connectorHandle</span><span class=o>=</span><span class=s1>&#39;HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}&#39;</span><span class=p>,</span> <span class=n>layout</span><span class=o>=</span><span class=s1>&#39;Optional[tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}]&#39;</span><span class=err>}</span><span class=p>,</span> <span class=n>grouped</span> <span class=o>=</span> <span class=k>false</span><span class=p>,</span> <span class=n>filterPredicate</span> <span class=o>=</span> <span class=n>shipdate</span> <span class=k>BETWEEN</span> <span class=p>(</span><span class=nb>DATE</span> <span class=mi>1992</span><span class=o>-</span><span class=mi>01</span><span class=o>-</span><span class=mi>01</span><span class=p>)</span> <span class=k>AND</span> <span class=p>(</span><span class=nb>DATE</span> <span class=mi>1992</span><span class=o>-</span><span class=mi>12</span><span class=o>-</span><span class=mi>31</span><span class=p>)]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>shipdate</span><span class=p>:</span><span class=nb>date</span><span class=p>]</span>
                <span class=n>Estimates</span><span class=p>:</span> <span class=err>{</span><span class=k>rows</span><span class=p>:</span> <span class=mi>600037902</span> <span class=p>(</span><span class=mi>2</span><span class=p>.</span><span class=mi>79</span><span class=n>GB</span><span class=p>),</span> <span class=n>cpu</span><span class=p>:</span> <span class=mi>3000189510</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>memory</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>network</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=err>}</span><span class=o>/</span><span class=err>{</span><span class=k>rows</span><span class=p>:</span> <span class=o>?</span> <span class=p>(</span><span class=o>?</span><span class=p>),</span> <span class=n>cpu</span><span class=p>:</span> <span class=mi>6000379020</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>memory</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>network</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=err>}</span>
                <span class=n>LAYOUT</span><span class=p>:</span> <span class=n>tpch</span><span class=p>.</span><span class=n>lineitem</span><span class=err>{</span><span class=n>domains</span><span class=o>=</span><span class=err>{</span><span class=n>shipdate</span><span class=o>=</span><span class=p>[</span> <span class=p>[[</span><span class=mi>1992</span><span class=o>-</span><span class=mi>01</span><span class=o>-</span><span class=mi>01</span><span class=p>,</span> <span class=mi>1992</span><span class=o>-</span><span class=mi>12</span><span class=o>-</span><span class=mi>31</span><span class=p>]]</span> <span class=p>]</span><span class=err>}}</span>
                <span class=n>shipdate</span> <span class=p>:</span><span class=o>=</span> <span class=n>shipdate</span><span class=p>:</span><span class=nb>date</span><span class=p>:</span><span class=mi>10</span><span class=p>:</span><span class=n>REGULAR</span>

</code></pre></div><p>Query plans are read bottom-up, starting with Fragment 1 that will scan the <code>lineitem</code> table, performing the filter on the <code>shipdate</code> column to apply the predicate. It will then perform a partial aggregation for each split, and exchange that partial result to the next stage <code>Fragment 0</code> to perform the final aggregation before delivering the results to the client.</p><div class=highlight><pre class=chroma><code class=language-SQL data-lang=SQL><span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>SELECT</span> <span class=k>COUNT</span><span class=p>(</span><span class=n>shipdate</span><span class=p>)</span> <span class=k>FROM</span> <span class=n>lineitem</span> <span class=k>WHERE</span> <span class=n>shipdate</span> <span class=k>BETWEEN</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-01-01&#39;</span> <span class=k>AND</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-12-31&#39;</span><span class=p>;</span>
  <span class=n>_col0</span>   
<span class=c1>----------
</span><span class=c1></span> <span class=mi>76036301</span>
<span class=p>(</span><span class=mi>1</span> <span class=k>row</span><span class=p>)</span>

<span class=n>Query</span> <span class=mi>20200609</span><span class=n>_154258_00019_ug2v4</span><span class=p>,</span> <span class=n>FINISHED</span><span class=p>,</span> <span class=mi>1</span> <span class=n>node</span>
<span class=n>Splits</span><span class=p>:</span> <span class=mi>367</span> <span class=n>total</span><span class=p>,</span> <span class=mi>367</span> <span class=n>done</span> <span class=p>(</span><span class=mi>100</span><span class=p>.</span><span class=mi>00</span><span class=o>%</span><span class=p>)</span>
<span class=mi>0</span><span class=p>:</span><span class=mi>09</span> <span class=p>[</span><span class=mi>600</span><span class=n>M</span> <span class=k>rows</span><span class=p>,</span> <span class=mi>928</span><span class=n>MB</span><span class=p>]</span> <span class=p>[</span><span class=mi>63</span><span class=p>.</span><span class=mi>2</span><span class=n>M</span> <span class=k>rows</span><span class=o>/</span><span class=n>s</span><span class=p>,</span> <span class=mi>97</span><span class=p>.</span><span class=mi>7</span><span class=n>MB</span><span class=o>/</span><span class=n>s</span><span class=p>]</span>
</code></pre></div><p>Running this query, we see there are a little over 76 million rows <code>lineitem</code> table in the year 1992. It took about 9 seconds to execute this query, processing 600 million rows.</p><p>Now let&rsquo;s set the session properties to enable the Aria features and take a look at the same explain plan.</p><div class=highlight><pre class=chroma><code class=language-SQL data-lang=SQL><span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>SET</span> <span class=k>SESSION</span> <span class=n>pushdown_subfields_enabled</span><span class=o>=</span><span class=k>true</span><span class=p>;</span>
<span class=k>SET</span> <span class=k>SESSION</span>
<span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>SET</span> <span class=k>SESSION</span> <span class=n>hive</span><span class=p>.</span><span class=n>pushdown_filter_enabled</span><span class=o>=</span><span class=k>true</span><span class=p>;</span>
<span class=k>SET</span> <span class=k>SESSION</span>
<span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>EXPLAIN</span> <span class=p>(</span><span class=k>TYPE</span> <span class=n>DISTRIBUTED</span><span class=p>)</span> <span class=k>SELECT</span> <span class=k>COUNT</span><span class=p>(</span><span class=n>shipdate</span><span class=p>)</span> <span class=k>FROM</span> <span class=n>lineitem</span> <span class=k>WHERE</span> <span class=n>shipdate</span> <span class=k>BETWEEN</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-01-01&#39;</span> <span class=k>AND</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-12-31&#39;</span><span class=p>;</span>
<span class=n>Fragment</span> <span class=mi>0</span> <span class=p>[</span><span class=n>SINGLE</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>layout</span><span class=p>:</span> <span class=p>[</span><span class=k>count</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>partitioning</span><span class=p>:</span> <span class=n>SINGLE</span> <span class=p>[]</span>
    <span class=n>Stage</span> <span class=n>Execution</span> <span class=n>Strategy</span><span class=p>:</span> <span class=n>UNGROUPED_EXECUTION</span>
    <span class=o>-</span> <span class=k>Output</span><span class=p>[</span><span class=n>_col0</span><span class=p>]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=k>count</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
            <span class=n>_col0</span> <span class=p>:</span><span class=o>=</span> <span class=k>count</span>
        <span class=o>-</span> <span class=k>Aggregate</span><span class=p>(</span><span class=k>FINAL</span><span class=p>)</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=k>count</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
                <span class=k>count</span> <span class=p>:</span><span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=n>presto</span><span class=p>.</span><span class=k>default</span><span class=p>.</span><span class=k>count</span><span class=s2>&#34;&#34;</span><span class=p>((</span><span class=n>count_4</span><span class=p>))</span>
            <span class=o>-</span> <span class=n>LocalExchange</span><span class=p>[</span><span class=n>SINGLE</span><span class=p>]</span> <span class=p>()</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
                <span class=o>-</span> <span class=n>RemoteSource</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>

<span class=n>Fragment</span> <span class=mi>1</span> <span class=p>[</span><span class=k>SOURCE</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>layout</span><span class=p>:</span> <span class=p>[</span><span class=n>count_4</span><span class=p>]</span>
    <span class=k>Output</span> <span class=n>partitioning</span><span class=p>:</span> <span class=n>SINGLE</span> <span class=p>[]</span>
    <span class=n>Stage</span> <span class=n>Execution</span> <span class=n>Strategy</span><span class=p>:</span> <span class=n>UNGROUPED_EXECUTION</span>
    <span class=o>-</span> <span class=k>Aggregate</span><span class=p>(</span><span class=k>PARTIAL</span><span class=p>)</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>count_4</span><span class=p>:</span><span class=nb>bigint</span><span class=p>]</span>
            <span class=n>count_4</span> <span class=p>:</span><span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=n>presto</span><span class=p>.</span><span class=k>default</span><span class=p>.</span><span class=k>count</span><span class=s2>&#34;&#34;</span><span class=p>((</span><span class=n>shipdate</span><span class=p>))</span>
        <span class=o>-</span> <span class=n>TableScan</span><span class=p>[</span><span class=n>TableHandle</span> <span class=err>{</span><span class=n>connectorId</span><span class=o>=</span><span class=s1>&#39;hive&#39;</span><span class=p>,</span> <span class=n>connectorHandle</span><span class=o>=</span><span class=s1>&#39;HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}&#39;</span><span class=p>,</span> <span class=n>layout</span><span class=o>=</span><span class=s1>&#39;Optional[tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}]&#39;</span><span class=err>}</span><span class=p>,</span> <span class=n>grouped</span> <span class=o>=</span> <span class=k>false</span><span class=p>]</span> <span class=o>=&gt;</span> <span class=p>[</span><span class=n>shipdate</span><span class=p>:</span><span class=nb>date</span><span class=p>]</span>
                <span class=n>Estimates</span><span class=p>:</span> <span class=err>{</span><span class=k>rows</span><span class=p>:</span> <span class=mi>540034112</span> <span class=p>(</span><span class=mi>2</span><span class=p>.</span><span class=mi>51</span><span class=n>GB</span><span class=p>),</span> <span class=n>cpu</span><span class=p>:</span> <span class=mi>2700170559</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>memory</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=p>,</span> <span class=n>network</span><span class=p>:</span> <span class=mi>0</span><span class=p>.</span><span class=mi>00</span><span class=err>}</span>
                <span class=n>LAYOUT</span><span class=p>:</span> <span class=n>tpch</span><span class=p>.</span><span class=n>lineitem</span><span class=err>{</span><span class=n>domains</span><span class=o>=</span><span class=err>{</span><span class=n>shipdate</span><span class=o>=</span><span class=p>[</span> <span class=p>[[</span><span class=mi>1992</span><span class=o>-</span><span class=mi>01</span><span class=o>-</span><span class=mi>01</span><span class=p>,</span> <span class=mi>1992</span><span class=o>-</span><span class=mi>12</span><span class=o>-</span><span class=mi>31</span><span class=p>]]</span> <span class=p>]</span><span class=err>}}</span>
                <span class=n>shipdate</span> <span class=p>:</span><span class=o>=</span> <span class=n>shipdate</span><span class=p>:</span><span class=nb>date</span><span class=p>:</span><span class=mi>10</span><span class=p>:</span><span class=n>REGULAR</span>
                    <span class=p>::</span> <span class=p>[[</span><span class=mi>1992</span><span class=o>-</span><span class=mi>01</span><span class=o>-</span><span class=mi>01</span><span class=p>,</span> <span class=mi>1992</span><span class=o>-</span><span class=mi>12</span><span class=o>-</span><span class=mi>31</span><span class=p>]]</span>
</code></pre></div><p>Note the major difference in the query plan at the very bottom, the inclusion of <code>shipdate</code> column in the <code>TableScan</code> operation. We see here that the connector now notices the predicate on the <code>shipdate</code> column of <code>1992-01-01</code> to <code>1992-12-31</code>. Let&rsquo;s give it a whirl.</p><div class=highlight><pre class=chroma><code class=language-SQL data-lang=SQL><span class=n>presto</span><span class=p>:</span><span class=n>tpch</span><span class=o>&gt;</span> <span class=k>SELECT</span> <span class=k>COUNT</span><span class=p>(</span><span class=n>shipdate</span><span class=p>)</span> <span class=k>FROM</span> <span class=n>lineitem</span> <span class=k>WHERE</span> <span class=n>shipdate</span> <span class=k>BETWEEN</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-01-01&#39;</span> <span class=k>AND</span> <span class=nb>DATE</span> <span class=s1>&#39;1992-12-31&#39;</span><span class=p>;</span>
  <span class=n>_col0</span>   
<span class=c1>----------
</span><span class=c1></span> <span class=mi>76036301</span>
<span class=p>(</span><span class=mi>1</span> <span class=k>row</span><span class=p>)</span>

<span class=n>Query</span> <span class=mi>20200609</span><span class=n>_154413_00023_ug2v4</span><span class=p>,</span> <span class=n>FINISHED</span><span class=p>,</span> <span class=mi>1</span> <span class=n>node</span>
<span class=n>Splits</span><span class=p>:</span> <span class=mi>367</span> <span class=n>total</span><span class=p>,</span> <span class=mi>367</span> <span class=n>done</span> <span class=p>(</span><span class=mi>100</span><span class=p>.</span><span class=mi>00</span><span class=o>%</span><span class=p>)</span>
<span class=mi>0</span><span class=p>:</span><span class=mi>05</span> <span class=p>[</span><span class=mi>76</span><span class=n>M</span> <span class=k>rows</span><span class=p>,</span> <span class=mi>928</span><span class=n>MB</span><span class=p>]</span> <span class=p>[</span><span class=mi>15</span><span class=p>.</span><span class=mi>5</span><span class=n>M</span> <span class=k>rows</span><span class=o>/</span><span class=n>s</span><span class=p>,</span> <span class=mi>189</span><span class=n>MB</span><span class=o>/</span><span class=n>s</span><span class=p>]</span>
</code></pre></div><p>We get the same result running the query, but the query time took almost half as long and, more importantly, we see only 76 million rows were scanned! The connector has applied the predicate on the <code>shipdate</code> column, rather than having the engine process the predicate. This saves some CPU cycles, resulting in faster query results. YMMV for your own queries and data sets, but if you&rsquo;re using the Hive connector with ORC files, it is definitely worth a look.</p><div class=nav-next-prev><div class=nav-prev><a class=grayed-out href=javascript:void()><i class="fas fa-chevron-left"></i></a></div><a class=nav-top href=#>top</i></a><div class=nav-next><a class=grayed-out href=javascript:void()><i class="fas fa-chevron-right"></i></a></div></div></div><footer><div class=footer-content><div class=contact-info><div class=footer-mail><i class="far fa-envelope"></i><a href=mailto:info@datacatessen.com>info@datacatessen.com</a></div></div><p class="copyright meta">Copyright © 2020 Datacatessen, LLC. All Rights Reserved.</p></div></footer></main></body><script src=https://datacatessen.com/js/navbutton.js></script></html>